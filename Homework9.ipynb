{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMjf0rTeoZeoul+qtGf5qvW",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/feiduobaby/good-first-issue/blob/main/Homework9.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "K695ulUaB751",
        "outputId": "e8e365ce-aad5-4743-f65b-5efe3454eb3e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2025-12-05 13:37:20--  https://github.com/alexeygrigorev/large-datasets/releases/download/hairstyle/hair_classifier_v1.onnx.data\n",
            "Resolving github.com (github.com)... 140.82.113.3\n",
            "Connecting to github.com (github.com)|140.82.113.3|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://release-assets.githubusercontent.com/github-production-release-asset/426348925/398ded4a-c41c-4e5a-9672-acb7e441de54?sp=r&sv=2018-11-09&sr=b&spr=https&se=2025-12-05T14%3A32%3A22Z&rscd=attachment%3B+filename%3Dhair_classifier_v1.onnx.data&rsct=application%2Foctet-stream&skoid=96c2d410-5711-43a1-aedd-ab1947aa7ab0&sktid=398a6654-997b-47e9-b12b-9515b896b4de&skt=2025-12-05T13%3A32%3A09Z&ske=2025-12-05T14%3A32%3A22Z&sks=b&skv=2018-11-09&sig=kp0CesACvxIYHUAdFYFqCnInClapK9tzi9epGSYIcHM%3D&jwt=eyJ0eXAiOiJKV1QiLCJhbGciOiJIUzI1NiJ9.eyJpc3MiOiJnaXRodWIuY29tIiwiYXVkIjoicmVsZWFzZS1hc3NldHMuZ2l0aHVidXNlcmNvbnRlbnQuY29tIiwia2V5Ijoia2V5MSIsImV4cCI6MTc2NDk0MzY0MCwibmJmIjoxNzY0OTQxODQwLCJwYXRoIjoicmVsZWFzZWFzc2V0cHJvZHVjdGlvbi5ibG9iLmNvcmUud2luZG93cy5uZXQifQ.4FqyX3qgi0UqwzGr4ESed4b7d097dwpz54yQF2N_fsE&response-content-disposition=attachment%3B%20filename%3Dhair_classifier_v1.onnx.data&response-content-type=application%2Foctet-stream [following]\n",
            "--2025-12-05 13:37:20--  https://release-assets.githubusercontent.com/github-production-release-asset/426348925/398ded4a-c41c-4e5a-9672-acb7e441de54?sp=r&sv=2018-11-09&sr=b&spr=https&se=2025-12-05T14%3A32%3A22Z&rscd=attachment%3B+filename%3Dhair_classifier_v1.onnx.data&rsct=application%2Foctet-stream&skoid=96c2d410-5711-43a1-aedd-ab1947aa7ab0&sktid=398a6654-997b-47e9-b12b-9515b896b4de&skt=2025-12-05T13%3A32%3A09Z&ske=2025-12-05T14%3A32%3A22Z&sks=b&skv=2018-11-09&sig=kp0CesACvxIYHUAdFYFqCnInClapK9tzi9epGSYIcHM%3D&jwt=eyJ0eXAiOiJKV1QiLCJhbGciOiJIUzI1NiJ9.eyJpc3MiOiJnaXRodWIuY29tIiwiYXVkIjoicmVsZWFzZS1hc3NldHMuZ2l0aHVidXNlcmNvbnRlbnQuY29tIiwia2V5Ijoia2V5MSIsImV4cCI6MTc2NDk0MzY0MCwibmJmIjoxNzY0OTQxODQwLCJwYXRoIjoicmVsZWFzZWFzc2V0cHJvZHVjdGlvbi5ibG9iLmNvcmUud2luZG93cy5uZXQifQ.4FqyX3qgi0UqwzGr4ESed4b7d097dwpz54yQF2N_fsE&response-content-disposition=attachment%3B%20filename%3Dhair_classifier_v1.onnx.data&response-content-type=application%2Foctet-stream\n",
            "Resolving release-assets.githubusercontent.com (release-assets.githubusercontent.com)... 185.199.108.133, 185.199.110.133, 185.199.111.133, ...\n",
            "Connecting to release-assets.githubusercontent.com (release-assets.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 80355328 (77M) [application/octet-stream]\n",
            "Saving to: ‘hair_classifier_v1.onnx.data’\n",
            "\n",
            "hair_classifier_v1. 100%[===================>]  76.63M   440MB/s    in 0.2s    \n",
            "\n",
            "2025-12-05 13:37:20 (440 MB/s) - ‘hair_classifier_v1.onnx.data’ saved [80355328/80355328]\n",
            "\n",
            "--2025-12-05 13:37:20--  https://github.com/alexeygrigorev/large-datasets/releases/download/hairstyle/hair_classifier_v1.onnx\n",
            "Resolving github.com (github.com)... 140.82.113.4\n",
            "Connecting to github.com (github.com)|140.82.113.4|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://release-assets.githubusercontent.com/github-production-release-asset/426348925/c6b83ad5-a901-40e9-bf2c-41ad174c870c?sp=r&sv=2018-11-09&sr=b&spr=https&se=2025-12-05T14%3A30%3A26Z&rscd=attachment%3B+filename%3Dhair_classifier_v1.onnx&rsct=application%2Foctet-stream&skoid=96c2d410-5711-43a1-aedd-ab1947aa7ab0&sktid=398a6654-997b-47e9-b12b-9515b896b4de&skt=2025-12-05T13%3A30%3A20Z&ske=2025-12-05T14%3A30%3A26Z&sks=b&skv=2018-11-09&sig=QdajjY75KhgGPj0yfcbNUpNFD%2BYTulo1orTWIYxRKgU%3D&jwt=eyJ0eXAiOiJKV1QiLCJhbGciOiJIUzI1NiJ9.eyJpc3MiOiJnaXRodWIuY29tIiwiYXVkIjoicmVsZWFzZS1hc3NldHMuZ2l0aHVidXNlcmNvbnRlbnQuY29tIiwia2V5Ijoia2V5MSIsImV4cCI6MTc2NDk0MjE0MCwibmJmIjoxNzY0OTQxODQwLCJwYXRoIjoicmVsZWFzZWFzc2V0cHJvZHVjdGlvbi5ibG9iLmNvcmUud2luZG93cy5uZXQifQ.KkrTfqr7KFjaFXKaUY817TRBtFpHOr9riOMORaPA8qM&response-content-disposition=attachment%3B%20filename%3Dhair_classifier_v1.onnx&response-content-type=application%2Foctet-stream [following]\n",
            "--2025-12-05 13:37:20--  https://release-assets.githubusercontent.com/github-production-release-asset/426348925/c6b83ad5-a901-40e9-bf2c-41ad174c870c?sp=r&sv=2018-11-09&sr=b&spr=https&se=2025-12-05T14%3A30%3A26Z&rscd=attachment%3B+filename%3Dhair_classifier_v1.onnx&rsct=application%2Foctet-stream&skoid=96c2d410-5711-43a1-aedd-ab1947aa7ab0&sktid=398a6654-997b-47e9-b12b-9515b896b4de&skt=2025-12-05T13%3A30%3A20Z&ske=2025-12-05T14%3A30%3A26Z&sks=b&skv=2018-11-09&sig=QdajjY75KhgGPj0yfcbNUpNFD%2BYTulo1orTWIYxRKgU%3D&jwt=eyJ0eXAiOiJKV1QiLCJhbGciOiJIUzI1NiJ9.eyJpc3MiOiJnaXRodWIuY29tIiwiYXVkIjoicmVsZWFzZS1hc3NldHMuZ2l0aHVidXNlcmNvbnRlbnQuY29tIiwia2V5Ijoia2V5MSIsImV4cCI6MTc2NDk0MjE0MCwibmJmIjoxNzY0OTQxODQwLCJwYXRoIjoicmVsZWFzZWFzc2V0cHJvZHVjdGlvbi5ibG9iLmNvcmUud2luZG93cy5uZXQifQ.KkrTfqr7KFjaFXKaUY817TRBtFpHOr9riOMORaPA8qM&response-content-disposition=attachment%3B%20filename%3Dhair_classifier_v1.onnx&response-content-type=application%2Foctet-stream\n",
            "Resolving release-assets.githubusercontent.com (release-assets.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.111.133, ...\n",
            "Connecting to release-assets.githubusercontent.com (release-assets.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 10337 (10K) [application/octet-stream]\n",
            "Saving to: ‘hair_classifier_v1.onnx’\n",
            "\n",
            "hair_classifier_v1. 100%[===================>]  10.09K  --.-KB/s    in 0s      \n",
            "\n",
            "2025-12-05 13:37:20 (87.1 MB/s) - ‘hair_classifier_v1.onnx’ saved [10337/10337]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "PREFIX=\"https://github.com/alexeygrigorev/large-datasets/releases/download/hairstyle\"\n",
        "DATA_URL=f\"{PREFIX}/hair_classifier_v1.onnx.data\"\n",
        "MODEL_URL=f\"{PREFIX}/hair_classifier_v1.onnx\"\n",
        "!wget $DATA_URL\n",
        "!wget $MODEL_URL"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install pillow"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AE4eAFQyDk6a",
        "outputId": "965e3741-de78-41fe-d48e-74fb5de13dcc"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pillow in /usr/local/lib/python3.12/dist-packages (11.3.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from io import BytesIO\n",
        "from urllib import request\n",
        "\n",
        "from PIL import Image\n",
        "\n",
        "def download_image(url):\n",
        "    with request.urlopen(url) as resp:\n",
        "        buffer = resp.read()\n",
        "    stream = BytesIO(buffer)\n",
        "    img = Image.open(stream)\n",
        "    return img\n",
        "\n",
        "\n",
        "def prepare_image(img, target_size):\n",
        "    if img.mode != 'RGB':\n",
        "        img = img.convert('RGB')\n",
        "    img = img.resize(target_size, Image.NEAREST)\n",
        "    return img"
      ],
      "metadata": {
        "id": "oGE_gfTNDp3h"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow import keras"
      ],
      "metadata": {
        "id": "R-Zuy7OeDrJR"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pip install onnx"
      ],
      "metadata": {
        "id": "o7pl6olGK-6K",
        "outputId": "fbf46a14-7e9c-4c35-d89c-86d4bb82f921",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting onnx\n",
            "  Downloading onnx-1.20.0-cp312-abi3-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (8.4 kB)\n",
            "Requirement already satisfied: numpy>=1.23.2 in /usr/local/lib/python3.12/dist-packages (from onnx) (2.0.2)\n",
            "Requirement already satisfied: protobuf>=4.25.1 in /usr/local/lib/python3.12/dist-packages (from onnx) (5.29.5)\n",
            "Requirement already satisfied: typing_extensions>=4.7.1 in /usr/local/lib/python3.12/dist-packages (from onnx) (4.15.0)\n",
            "Requirement already satisfied: ml_dtypes>=0.5.0 in /usr/local/lib/python3.12/dist-packages (from onnx) (0.5.4)\n",
            "Downloading onnx-1.20.0-cp312-abi3-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (18.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m18.1/18.1 MB\u001b[0m \u001b[31m96.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: onnx\n",
            "Successfully installed onnx-1.20.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import onnx\n",
        "\n",
        "# Load your ONNX model\n",
        "model = onnx.load(\"/content/hair_classifier_v1.onnx\")\n",
        "\n",
        "# Get the output names\n",
        "output_names = [output.name for output in model.graph.output]\n",
        "\n",
        "print(\"Output names:\", output_names)\n"
      ],
      "metadata": {
        "id": "4Ws2i5--JvUt",
        "outputId": "a99ec0e0-832d-4d95-ed38-ab06b0a22e32",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Output names: ['output']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!wget 'https://habrastorage.org/webt/yf/_d/ok/yf_dokzqy3vcritme8ggnzqlvwa.jpeg'"
      ],
      "metadata": {
        "id": "l1iSx2680gdY",
        "outputId": "12a48f92-d626-4064-a3a9-227a9e0eafa3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2025-12-05 13:38:08--  https://habrastorage.org/webt/yf/_d/ok/yf_dokzqy3vcritme8ggnzqlvwa.jpeg\n",
            "Resolving habrastorage.org (habrastorage.org)... 95.47.173.35, 95.47.173.34, 2a14:b680:0:56::34, ...\n",
            "Connecting to habrastorage.org (habrastorage.org)|95.47.173.35|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 398272 (389K) [image/jpeg]\n",
            "Saving to: ‘yf_dokzqy3vcritme8ggnzqlvwa.jpeg’\n",
            "\n",
            "yf_dokzqy3vcritme8g 100%[===================>] 388.94K  1.07MB/s    in 0.4s    \n",
            "\n",
            "2025-12-05 13:38:09 (1.07 MB/s) - ‘yf_dokzqy3vcritme8ggnzqlvwa.jpeg’ saved [398272/398272]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "img = prepare_image(download_image('https://habrastorage.org/webt/yf/_d/ok/yf_dokzqy3vcritme8ggnzqlvwa.jpeg'),(200,200))"
      ],
      "metadata": {
        "id": "mLXmtquw8Miy"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "\n",
        "SEED = 42\n",
        "np.random.seed(SEED)\n",
        "torch.manual_seed(SEED)\n",
        "\n",
        "if torch.cuda.is_available():\n",
        "    torch.cuda.manual_seed(SEED)\n",
        "    torch.cuda.manual_seed_all(SEED)\n",
        "\n",
        "torch.backends.cudnn.deterministic = True\n",
        "torch.backends.cudnn.benchmark = False"
      ],
      "metadata": {
        "id": "si3xpGJPAI46"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "class SimpleCNN(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        # 1) Conv layer: in 3 ch, out 32 ch, kernel 3x3\n",
        "        self.conv1 = nn.Conv2d(in_channels=3, out_channels=32, kernel_size=(3, 3))\n",
        "        # 2) Max pooling: 2x2\n",
        "        self.pool = nn.MaxPool2d(kernel_size=(2, 2))\n",
        "        # After conv1 (no padding): input (3,200,200) -> (32, 198, 198)\n",
        "        # After maxpool 2x2: -> (32, 99, 99)\n",
        "        # Flatten size = 32 * 99 * 99\n",
        "        self.fc1 = nn.Linear(32 * 99 * 99, 64)\n",
        "        self.fc2 = nn.Linear(64, 1)\n",
        "        self.sigmoid = nn.Sigmoid()  # binary output\n",
        "\n",
        "    def forward(self, x):\n",
        "        # x: (batch, 3, 200, 200)\n",
        "        x = self.conv1(x)          # (batch, 32, 198, 198)\n",
        "        x = F.relu(x)\n",
        "        x = self.pool(x)           # (batch, 32, 99, 99)\n",
        "        x = torch.flatten(x, 1)    # (batch, 32*99*99)\n",
        "        x = self.fc1(x)            # (batch, 64)\n",
        "        x = F.relu(x)\n",
        "        x = self.fc2(x)            # (batch, 1)\n",
        "        x = self.sigmoid(x)        # probability for class 1\n",
        "        return x"
      ],
      "metadata": {
        "id": "OK6COaEKCpBM"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = SimpleCNN()\n",
        "\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr=0.002, momentum=0.8)\n",
        "\n",
        "criterion = nn.MSELoss()"
      ],
      "metadata": {
        "id": "94u3Iu8ACrMl"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from PIL import Image\n",
        "import torch\n",
        "from torchvision import transforms\n",
        "\n",
        "# 1) Define same size, channels-first, and normalization if you want\n",
        "transform = transforms.Compose([\n",
        "    transforms.Resize((200, 200)),     # model expects 200x200 [web:35]\n",
        "    transforms.ToTensor(),             # -> (C, H, W), values in [0, 1] [web:36]\n",
        "    # Optional: add Normalize(...) here if you trained with it [web:39]\n",
        "])\n",
        "\n",
        "# 2) Load and transform image\n",
        "img = img.convert(\"RGB\")  # 3 channels [web:37]\n",
        "x = transform(img)            # shape (3, 200, 200)\n",
        "x = x.unsqueeze(0)            # shape (1, 3, 200, 200) add batch dim [web:35]"
      ],
      "metadata": {
        "id": "OJSwlo4wDjw9"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = SimpleCNN()\n",
        "model.eval()                                                            # eval mode [web:37]\n",
        "\n",
        "with torch.no_grad():\n",
        "    y = model(x)              # shape (1, 1)\n",
        "    prob = y.item()           # scalar probability for class 1\n",
        "\n",
        "# For binary decision:\n",
        "pred = 1 if prob >= 0.5 else 0\n",
        "print(\"prob:\", prob, \"pred:\", pred)"
      ],
      "metadata": {
        "id": "X7sRELQVESxA",
        "outputId": "5664b68a-d8e1-4dd8-ae31-79e2d48ab6d8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "prob: 0.4977841079235077 pred: 0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%shell\n",
        "pip install udocker\n",
        "udocker --allow-root install"
      ],
      "metadata": {
        "id": "F7hNBwn4WzIY",
        "outputId": "bba1f9b1-1976-4b6e-d616-835fac898709",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting udocker\n",
            "  Downloading udocker-1.3.17-py2.py3-none-any.whl.metadata (37 kB)\n",
            "Downloading udocker-1.3.17-py2.py3-none-any.whl (119 kB)\n",
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/119.6 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m119.6/119.6 kB\u001b[0m \u001b[31m9.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: udocker\n",
            "Successfully installed udocker-1.3.17\n",
            "Info: creating repo: /root/.udocker\n",
            "Info: udocker command line interface 1.3.17\n",
            "Info: searching for udockertools >= 1.2.11\n",
            "Info: installing udockertools 1.2.11\n",
            "Info: installation of udockertools successful\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": []
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "\n",
        "repo = \"agrigorev/model-2025-hairstyle\"\n",
        "tag = \"v1\"\n",
        "\n",
        "# 获取 token\n",
        "auth_url = f\"https://auth.docker.io/token?service=registry.docker.io&scope=repository:{repo}:pull\"\n",
        "token = requests.get(auth_url).json()[\"token\"]\n",
        "\n",
        "# 获取 manifest\n",
        "headers = {\n",
        "    \"Authorization\": f\"Bearer {token}\",\n",
        "    \"Accept\": \"application/vnd.docker.distribution.manifest.v2+json\"\n",
        "}\n",
        "manifest_url = f\"https://registry-1.docker.io/v2/{repo}/manifests/{tag}\"\n",
        "manifest = requests.get(manifest_url, headers=headers).json()\n",
        "\n",
        "# 输出 digest\n",
        "digest = manifest[\"config\"][\"digest\"]\n",
        "print(\"Manifest digest:\", digest)\n",
        "\n",
        "# 计算镜像总大小\n",
        "total_size = sum(layer[\"size\"] for layer in manifest[\"layers\"])\n",
        "print(\"Total size (bytes):\", total_size)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aYIgx_-qW39a",
        "outputId": "14c2a92b-aa9e-485d-9bdd-e2364229ae5f"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Manifest digest: sha256:4528ad1525d55b7b0c0872d6b7882b654162fcc05a013e455719ac0c0eb44ad3\n",
            "Total size (bytes): 268965283\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for layer in manifest[\"layers\"]:\n",
        "    layer_digest = layer[\"digest\"]\n",
        "    url = f\"https://registry-1.docker.io/v2/{repo}/blobs/{layer_digest}\"\n",
        "    headers = {\"Authorization\": f\"Bearer {token}\"}\n",
        "    r = requests.get(url, headers=headers, stream=True)\n",
        "    layer_file = layer_digest.replace(\":\", \"_\") + \".tar.gz\"\n",
        "    with open(layer_file, \"wb\") as f:\n",
        "        for chunk in r.iter_content(1024*1024):\n",
        "            f.write(chunk)\n",
        "    print(f\"Saved {layer_file}\")"
      ],
      "metadata": {
        "id": "oPUXK7E0qtjf",
        "outputId": "e8f1694e-6841-469a-c76c-7838a83295ac",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saved sha256_b71754c34aa30fa068984c76f3eaf68b76ee2e54328b98dcf2b30a00786c2fd0.tar.gz\n",
            "Saved sha256_d2379533db7f1f8d28362676e66255b190981a48c09f71c5349d1c430b486da2.tar.gz\n",
            "Saved sha256_26c6d8a1e1c2efebb092f81375307e44c7a24372b57f330bbc8cf04aa0673ec4.tar.gz\n",
            "Saved sha256_94f2ecca3b37ec53066991d968d105de9ec006f858ca315b4349d27072eea86e.tar.gz\n",
            "Saved sha256_f9e394d707b7723a883f06ec18a667e9dfb92afb8adee558b6002a0baed96df1.tar.gz\n",
            "Saved sha256_15b946295de28858d1ff78561548e1daa544168847ff31cef0a10e2b18a50ff5.tar.gz\n",
            "Saved sha256_55d9a27bb275e3e351c7135a444865ff4267198ee09af922f6776a8a42633abc.tar.gz\n",
            "Saved sha256_cd9c39ee4ab89af1dee8507f2bde9f1c1e503cb515fcd91daebbb8ac52b910c6.tar.gz\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import tarfile\n",
        "import os\n",
        "\n",
        "layer_files = [\n",
        "    \"/content/sha256_15b946295de28858d1ff78561548e1daa544168847ff31cef0a10e2b18a50ff5.tar.gz\",\n",
        "    \"/content/sha256_26c6d8a1e1c2efebb092f81375307e44c7a24372b57f330bbc8cf04aa0673ec4.tar.gz\",\n",
        "    \"/content/sha256_55d9a27bb275e3e351c7135a444865ff4267198ee09af922f6776a8a42633abc.tar.gz\",\n",
        "    \"/content/sha256_94f2ecca3b37ec53066991d968d105de9ec006f858ca315b4349d27072eea86e.tar.gz\",\n",
        "    \"/content/sha256_b71754c34aa30fa068984c76f3eaf68b76ee2e54328b98dcf2b30a00786c2fd0.tar.gz\",\n",
        "    \"/content/sha256_cd9c39ee4ab89af1dee8507f2bde9f1c1e503cb515fcd91daebbb8ac52b910c6.tar.gz\",\n",
        "    \"/content/sha256_d2379533db7f1f8d28362676e66255b190981a48c09f71c5349d1c430b486da2.tar.gz\",\n",
        "    \"/content/sha256_f9e394d707b7723a883f06ec18a667e9dfb92afb8adee558b6002a0baed96df1.tar.gz\",\n",
        "]\n",
        "\n",
        "# 解压到同一个文件夹，后面的 layer 会覆盖前面的\n",
        "os.makedirs(\"/content/full_image\", exist_ok=True)\n",
        "\n",
        "for layer_file in layer_files:\n",
        "    print(f\"Extracting {layer_file}...\")\n",
        "    with tarfile.open(layer_file, \"r:\") as tar:\n",
        "        tar.extractall(path=\"/content/full_image\")"
      ],
      "metadata": {
        "id": "BTjc8xAgrnJH",
        "outputId": "5b252ab2-55a4-4b35-f2a8-3691f158b26d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 365
        }
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting /content/sha256_15b946295de28858d1ff78561548e1daa544168847ff31cef0a10e2b18a50ff5.tar.gz...\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "ReadError",
          "evalue": "truncated header",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mReadError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-3932812022.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mlayer_file\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mlayer_files\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Extracting {layer_file}...\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m     \u001b[0;32mwith\u001b[0m \u001b[0mtarfile\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlayer_file\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"r:\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mtar\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     21\u001b[0m         \u001b[0mtar\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextractall\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"/content/full_image\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.12/tarfile.py\u001b[0m in \u001b[0;36mopen\u001b[0;34m(cls, name, mode, fileobj, bufsize, **kwargs)\u001b[0m\n\u001b[1;32m   1873\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1874\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mCompressionError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"unknown compression type %r\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mcomptype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1875\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfilemode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfileobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1876\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1877\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0;34m\"|\"\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.12/tarfile.py\u001b[0m in \u001b[0;36mtaropen\u001b[0;34m(cls, name, mode, fileobj, **kwargs)\u001b[0m\n\u001b[1;32m   1905\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mmode\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m\"r\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"a\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"w\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"x\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1906\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"mode must be 'r', 'a', 'w' or 'x'\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1907\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfileobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1908\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1909\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mclassmethod\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.12/tarfile.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, name, mode, fileobj, format, tarinfo, dereference, ignore_zeros, encoding, errors, pax_headers, debug, errorlevel, copybufsize)\u001b[0m\n\u001b[1;32m   1763\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"r\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1764\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfirstmember\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1765\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfirstmember\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1766\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1767\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"a\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.12/tarfile.py\u001b[0m in \u001b[0;36mnext\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   2757\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mTruncatedHeaderError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2758\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moffset\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2759\u001b[0;31m                     \u001b[0;32mraise\u001b[0m \u001b[0mReadError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2760\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mSubsequentHeaderError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2761\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mReadError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mReadError\u001b[0m: truncated header"
          ]
        }
      ]
    }
  ]
}